{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":497494,"sourceType":"datasetVersion","datasetId":233357}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom PIL import Image\nimport h5py\nimport torch.nn.functional as F\nimport random\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False  # This will make training slower, but more reproducible\n\nset_seed(38) \n\n# Custom Dataset Class to Handle Images and Precomputed Density Maps\nclass CrowdDataset(Dataset):\n    def __init__(self, image_dir, density_dir, transform=None):\n        self.image_dir = image_dir\n        self.density_dir = density_dir\n        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n        density_path = os.path.join(self.density_dir, self.image_filenames[idx].replace('.jpg', '.h5'))\n\n        # Load image\n        image = Image.open(img_path).convert('RGB')\n\n        # Load density map from .h5 file\n        with h5py.File(density_path, 'r') as hf:\n            density_map = np.array(hf['density'])\n\n        # Apply transforms to the image\n        if self.transform:\n            image = self.transform(image)\n\n        # Convert density map to a tensor\n        density_map = torch.from_numpy(density_map).unsqueeze(0).float()  # Add channel dimension\n\n        return image, density_map\n\n\n# D-ConvNet-v1 Implementation\nclass DConvNet_v1(nn.Module):\n    def __init__(self, pretrained=True, num_regressors=3):\n        super(DConvNet_v1, self).__init__()\n\n        # Load the VGG16 model\n        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n\n        # Modify the feature extractor (up to conv4_3)\n        self.features = nn.Sequential(*list(vgg16.features.children())[:23])  # Up to conv4_3\n\n        # Modify the fourth pooling layer (set stride to 1)\n        self.features.add_module('pool4', nn.MaxPool2d(kernel_size=2, stride=1, padding=0))\n\n        # Add dilated convolutions in place of the fifth pooling layer\n        self.features.add_module('dilated_conv5_1', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('relu5_1', nn.ReLU(inplace=True))\n        self.features.add_module('dilated_conv5_2', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('relu5_2', nn.ReLU(inplace=True))\n        self.features.add_module('dilated_conv5_3', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('relu5_3', nn.ReLU(inplace=True))\n\n        # Define group convolutional layers for regression with dropout\n        self.regressors = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(512, 64, kernel_size=1, groups=64),  # Group convolution\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.3),  # Added dropout layer\n                nn.Conv2d(64, 1, kernel_size=1)  # Final 1x1 convolution to get the density map\n            ) for _ in range(num_regressors)\n        ])\n\n    def forward(self, x):\n        # Pass through the modified VGG16 feature extractor\n        x = self.features(x)\n        \n        # Apply the group convolutional regressors\n        outputs = [regressor(x) for regressor in self.regressors]\n        return outputs\n\n\n# Loss Function (Euclidean Loss + Negative Correlation)\ndef negative_correlation_loss(outputs, target, lambda_param=0.001):\n    mse_loss = nn.MSELoss()\n    target = target.to(outputs[0].device)\n\n    # Upsample to match the target size\n    total_mse = sum([mse_loss(F.interpolate(output, size=target.shape[2:], mode='bilinear', align_corners=False), target) for output in outputs]) / len(outputs)\n\n    # Calculate pairwise correlations between regressors\n    correlations = []\n    for i in range(len(outputs)):\n        for j in range(i + 1, len(outputs)):\n            o_i = outputs[i].view(-1)\n            o_j = outputs[j].view(-1)\n            corr = torch.corrcoef(torch.stack([o_i, o_j]))[0, 1]\n            correlations.append(corr)\n\n    correlation_penalty = -sum(correlations) / (len(correlations) + 1e-8) if correlations else 0\n    return total_mse + lambda_param * correlation_penalty\n\n\n# Optimizer Function\ndef get_optimizer(model):\n    return optim.SGD([\n        {'params': model.features.parameters(), 'lr': 1e-5},  # Smaller learning rate for feature extraction layers\n        {'params': model.regressors.parameters(), 'lr': 1e-4}  # Larger learning rate for regressor layers\n    ], momentum=0.9, weight_decay=1e-3)  # Increased weight decay to 1e-3\n\n# Learning Rate Scheduler\ndef get_scheduler(optimizer):\n    return optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n\n# Data augmentation and normalization\ndata_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),  # Increased rotation range\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Random cropping\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Brightness and contrast adjustments\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\n\n# Custom collate function to resize images and density maps in each batch\ndef custom_collate(batch):\n    max_height = max([item[0].shape[1] for item in batch])\n    max_width = max([item[0].shape[2] for item in batch])\n\n    resized_images = []\n    resized_density_maps = []\n    for image, density_map in batch:\n        image = F.interpolate(image.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False)\n        image = image.squeeze(0)\n\n        density_map = F.interpolate(density_map.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False)\n        density_map = density_map.squeeze(0)\n\n        resized_images.append(image)\n        resized_density_maps.append(density_map)\n\n    return torch.stack(resized_images), torch.stack(resized_density_maps)\n\n\n# Training Loop with Model Saving and Evaluation\ndef train_model(model, train_dataloader, test_dataloader, num_epochs=20, lambda_param=0.001, save_path='model_checkpoint.pth'):\n    model = model.to(device)\n    optimizer = get_optimizer(model)\n    scheduler = get_scheduler(optimizer)\n    best_mae = float('inf')\n    early_stop_patience = 5  # Increased patience for early stopping\n    no_improve_epochs = 0\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        for images, density_maps in train_dataloader:\n            images = images.to(device)\n            density_maps = density_maps.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)  # Forward pass\n            loss = negative_correlation_loss(outputs, density_maps, lambda_param)  # Loss\n            loss.backward()  # Backward pass\n            optimizer.step()  # Update weights\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n\n        # Evaluate the model after each epoch\n        model.eval()\n        mae, rmse = evaluate_model(model, test_dataloader)\n\n        # Save the model if it improves\n        if mae < best_mae:\n            best_mae = mae\n            no_improve_epochs = 0\n            torch.save(model.state_dict(), f'{save_path}_epoch_{epoch+1}.pth')\n            print(f\"Model saved as {save_path}_epoch_{epoch+1}.pth\")\n        else:\n            no_improve_epochs += 1\n\n        # Early stopping\n        if no_improve_epochs >= early_stop_patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n        # Step the scheduler\n        scheduler.step()\n\n\n# Evaluation Function\ndef evaluate_model(model, dataloader):\n    model.eval()\n    mae, rmse = 0.0, 0.0\n    with torch.no_grad():\n        for images, density_maps in dataloader:\n            images = images.to(device)\n            density_maps = density_maps.to(device)\n\n            outputs = model(images)  # Forward pass\n            avg_output = sum(outputs) / len(outputs)\n\n            mae += torch.abs(avg_output.sum() - density_maps.sum()).item()\n            rmse += ((avg_output.sum() - density_maps.sum()) ** 2).item()\n\n    mae /= len(dataloader)\n    rmse = (rmse / len(dataloader)) ** 0.5\n    print(f\"Validation MAE: {mae}, Validation RMSE: {rmse}\")\n    return mae, rmse\n\n\n# Configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Dataset paths (Update these to your local paths)\ntrain_image_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_B/train_data/images'\ntrain_density_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_B/train_data/ground-truth-h5'\n\ntest_image_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_B/test_data/images'\ntest_density_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_B/test_data/ground-truth-h5'\n\n# Initialize the Dataset and DataLoader\ntrain_dataset = CrowdDataset(train_image_dir, train_density_dir, transform=data_transforms)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=custom_collate)\n\ntest_dataset = CrowdDataset(test_image_dir, test_density_dir, transform=data_transforms)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=custom_collate)\n\n# Model Initialization\nmodel = DConvNet_v1(pretrained=True)\n\n# Training the model and evaluating after each epoch\ntrain_model(model, train_dataloader, test_dataloader, num_epochs=20, lambda_param=0.001, save_path='model_checkpoint')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-22T03:11:35.530171Z","iopub.execute_input":"2024-10-22T03:11:35.530617Z","iopub.status.idle":"2024-10-22T03:19:02.281016Z","shell.execute_reply.started":"2024-10-22T03:11:35.530556Z","shell.execute_reply":"2024-10-22T03:19:02.279848Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 224MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 0.004469093352090567\nValidation MAE: 85.8977326984647, Validation RMSE: 86.69504490549015\nModel saved as model_checkpoint_epoch_1.pth\nEpoch 2/20, Loss: 0.003640223166439682\nValidation MAE: 71.2718930787678, Validation RMSE: 72.38968757589605\nModel saved as model_checkpoint_epoch_2.pth\nEpoch 3/20, Loss: 0.0031663927901536227\nValidation MAE: 61.03608066220827, Validation RMSE: 62.21734985703183\nModel saved as model_checkpoint_epoch_3.pth\nEpoch 4/20, Loss: 0.002910363390110433\nValidation MAE: 53.600651173651976, Validation RMSE: 54.881849279522186\nModel saved as model_checkpoint_epoch_4.pth\nEpoch 5/20, Loss: 0.002750130305066705\nValidation MAE: 47.98938973342316, Validation RMSE: 49.58911963925525\nModel saved as model_checkpoint_epoch_5.pth\nEpoch 6/20, Loss: 0.002679771021939814\nValidation MAE: 45.943983560876, Validation RMSE: 47.515429783845086\nModel saved as model_checkpoint_epoch_6.pth\nEpoch 7/20, Loss: 0.0026428696955554188\nValidation MAE: 43.91480182695992, Validation RMSE: 45.503078978945005\nModel saved as model_checkpoint_epoch_7.pth\nEpoch 8/20, Loss: 0.002638260410167277\nValidation MAE: 43.25091103662418, Validation RMSE: 44.891850345109646\nModel saved as model_checkpoint_epoch_8.pth\nEpoch 9/20, Loss: 0.0025801704172044994\nValidation MAE: 40.4535989519916, Validation RMSE: 42.270878398797215\nModel saved as model_checkpoint_epoch_9.pth\nEpoch 10/20, Loss: 0.002575211557559669\nValidation MAE: 40.1192019981674, Validation RMSE: 42.04032307440478\nModel saved as model_checkpoint_epoch_10.pth\nEpoch 11/20, Loss: 0.002530579222366214\nValidation MAE: 39.49227137505254, Validation RMSE: 41.42839849399915\nModel saved as model_checkpoint_epoch_11.pth\nEpoch 12/20, Loss: 0.002541722855530679\nValidation MAE: 39.004623630378816, Validation RMSE: 40.849355186093334\nModel saved as model_checkpoint_epoch_12.pth\nEpoch 13/20, Loss: 0.0025356265576556326\nValidation MAE: 38.65487943721723, Validation RMSE: 40.354352423479604\nModel saved as model_checkpoint_epoch_13.pth\nEpoch 14/20, Loss: 0.0025365304457955064\nValidation MAE: 38.37254893628857, Validation RMSE: 40.204110954893295\nModel saved as model_checkpoint_epoch_14.pth\nEpoch 15/20, Loss: 0.0025242024636827408\nValidation MAE: 37.35338643230969, Validation RMSE: 39.34655670090869\nModel saved as model_checkpoint_epoch_15.pth\nEpoch 16/20, Loss: 0.00251290823565796\nValidation MAE: 37.46450441094893, Validation RMSE: 39.521844359919754\nEpoch 17/20, Loss: 0.002508226316422224\nValidation MAE: 36.76955536950992, Validation RMSE: 38.734333843843345\nModel saved as model_checkpoint_epoch_17.pth\nEpoch 18/20, Loss: 0.002505697312299162\nValidation MAE: 36.860031731521026, Validation RMSE: 38.946133370223805\nEpoch 19/20, Loss: 0.0025312924408353865\nValidation MAE: 36.888048968737635, Validation RMSE: 38.86214665207695\nEpoch 20/20, Loss: 0.0025004889979027213\nValidation MAE: 36.13682484928566, Validation RMSE: 38.264571308419434\nModel saved as model_checkpoint_epoch_20.pth\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip freeze > requirements.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T03:22:46.041310Z","iopub.execute_input":"2024-10-22T03:22:46.042114Z","iopub.status.idle":"2024-10-22T03:22:49.333882Z","shell.execute_reply.started":"2024-10-22T03:22:46.042075Z","shell.execute_reply":"2024-10-22T03:22:49.332750Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"print(\"Torch:\", torch.__version__)\nprint(\"Torchvision:\", torchvision.__version__)\nprint(\"h5py:\", h5py.__version__)\nprint(\"Pillow:\", PIL.__version__)\nprint(\"scikit-learn:\", sklearn.__version__)\nprint(\"NumPy:\", np.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-22T03:33:13.801694Z","iopub.execute_input":"2024-10-22T03:33:13.802579Z","iopub.status.idle":"2024-10-22T03:33:14.292482Z","shell.execute_reply.started":"2024-10-22T03:33:13.802528Z","shell.execute_reply":"2024-10-22T03:33:14.291102Z"}},"outputs":[{"name":"stdout","text":"Torch: 2.4.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchvision:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mtorchvision\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py:\u001b[39m\u001b[38;5;124m\"\u001b[39m, h5py\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPillow:\u001b[39m\u001b[38;5;124m\"\u001b[39m, PIL\u001b[38;5;241m.\u001b[39m__version__)\n","\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"],"ename":"NameError","evalue":"name 'torchvision' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}