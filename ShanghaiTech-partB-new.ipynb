{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":497494,"sourceType":"datasetVersion","datasetId":233357}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nfrom PIL import Image\nimport h5py\nimport torch.nn.functional as F\nimport random\n\n# Set random seed for reproducibility\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(38)\n\n# Dataset definition\nclass CrowdDataset(Dataset):\n    def __init__(self, image_dir, density_dir, transform=None):\n        self.image_dir = image_dir\n        self.density_dir = density_dir\n        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n        density_path = os.path.join(self.density_dir, self.image_filenames[idx].replace('.jpg', '.h5'))\n\n        image = Image.open(img_path).convert('RGB')\n        with h5py.File(density_path, 'r') as hf:\n            density_map = np.array(hf['density'])\n\n        if self.transform:\n            image = self.transform(image)\n        density_map = torch.from_numpy(density_map).unsqueeze(0).float() \n\n        return image, density_map\n\n# MultiScale Attention with Adaptive Scale Weights\nclass MultiScaleAttention(nn.Module):\n    def __init__(self):\n        super(MultiScaleAttention, self).__init__()\n        \n        # Multi-Scale Feature Extraction with adaptive scale weights\n        self.conv1 = nn.Conv2d(512, 128, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(512, 128, kernel_size=5, padding=2)\n        self.conv3 = nn.Conv2d(512, 128, kernel_size=7, padding=3)\n        self.conv4 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n        \n        # Adaptive scale weights\n        self.scale_weights = nn.Parameter(torch.ones(4))\n\n        # Channel Attention Mechanism\n        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.global_max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc1 = nn.Linear(512, 128)\n        self.fc2 = nn.Linear(128, 512)\n        \n        # Enhanced Spatial Attention Mechanism\n        self.spatial_conv1 = nn.Conv2d(512, 512, kernel_size=1)\n        self.spatial_conv2 = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n\n    def forward(self, x):\n        # Multi-Scale Feature Extraction with Adaptive Scale Weighting\n        f1 = self.conv1(x)\n        f2 = self.conv2(x)\n        f3 = self.conv3(x)\n        f4 = self.conv4(x)\n        \n        # Apply scale weights and concatenate features\n        f_multi = torch.cat([f * self.scale_weights[i] for i, f in enumerate([f1, f2, f3, f4])], dim=1)\n        \n        # Channel Attention\n        avg_pool = self.global_avg_pool(f_multi).view(f_multi.size(0), -1)\n        max_pool = self.global_max_pool(f_multi).view(f_multi.size(0), -1)\n        channel_weights = torch.sigmoid(self.fc2(F.relu(self.fc1(avg_pool + max_pool)))).view(f_multi.size(0), 512, 1, 1)\n        f_channel = f_multi * channel_weights\n        \n        # Enhanced Spatial Attention\n        f_spatial = F.relu(self.spatial_conv1(f_channel))\n        avg_out = torch.mean(f_spatial, dim=1, keepdim=True)\n        max_out, _ = torch.max(f_spatial, dim=1, keepdim=True)\n        spatial_attention = torch.sigmoid(self.spatial_conv2(torch.cat([avg_out, max_out], dim=1)))\n        f_attention = f_spatial * spatial_attention\n        \n        # Residual Connection\n        f_attention += f_channel\n        \n        return f_attention\n\n# Main model with attention mechanism and dilated regressors\nclass DConvNet_v1_with_Attention(nn.Module):\n    def __init__(self, pretrained=True, num_regressors=5):\n        super(DConvNet_v1_with_Attention, self).__init__()\n        \n        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        self.features = nn.Sequential(*list(vgg16.features.children())[:23])\n        self.features.add_module('pool4', nn.MaxPool2d(kernel_size=2, stride=1, padding=0))\n        self.features.add_module('dilated_conv5_1', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('bn5_1', nn.BatchNorm2d(512))\n        self.features.add_module('relu5_1', nn.ReLU(inplace=True))\n        self.features.add_module('dilated_conv5_2', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('bn5_2', nn.BatchNorm2d(512))\n        self.features.add_module('relu5_2', nn.ReLU(inplace=True))\n        self.features.add_module('dilated_conv5_3', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('bn5_3', nn.BatchNorm2d(512))\n        self.features.add_module('relu5_3', nn.ReLU(inplace=True))\n\n        # Unfreeze additional VGG layers for fine-tuning\n        for param in self.features[:10].parameters():\n            param.requires_grad = True\n\n        self.attention = MultiScaleAttention()\n\n        # Dilated Convolution Regressors\n        self.regressors = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(512, 64, kernel_size=1, groups=64),\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.05),\n                nn.Conv2d(64, 64, kernel_size=3, padding=2, dilation=2),\n                nn.Conv2d(64, 1, kernel_size=1)\n            ) for _ in range(num_regressors)\n        ])\n        self.regressors.apply(self.init_weights)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.attention(x)\n        outputs = [regressor(x) for regressor in self.regressors]\n        return outputs\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Conv2d):\n            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n            if m.bias is not None:\n                m.bias.data.fill_(0.01)\n\n# Improved Negative Correlation Loss with Weighted MSE for Crowded Areas\ndef negative_correlation_loss(outputs, target, lambda_param=0.0001):\n    target = target.to(outputs[0].device)\n    mse_loss = nn.MSELoss(reduction='none')\n    \n    total_mse = sum([mse_loss(F.interpolate(output, size=target.shape[2:], mode='bilinear', align_corners=False), target) for output in outputs]) / len(outputs)\n\n    # Compute weighted loss for crowded areas\n    crowded_area_weight = 10\n    weight_map = torch.where(target > 0.1, crowded_area_weight, 1.0).to(target.device)\n    weighted_mse = (weight_map * total_mse).mean()\n    \n    # Adaptive lambda: decay over epochs (can adjust for time decay)\n    adaptive_lambda = lambda_param\n    correlations = []\n    for i in range(len(outputs)):\n        for j in range(i + 1, len(outputs)):\n            o_i = outputs[i].view(-1)\n            o_j = outputs[j].view(-1)\n            corr = torch.corrcoef(torch.stack([o_i, o_j]))[0, 1]\n            correlations.append(corr)\n\n    correlation_penalty = abs(sum(correlations)) / (len(correlations) + 1e-8) if correlations else 0\n    return weighted_mse + adaptive_lambda * correlation_penalty\n\n# Optimizer and scheduler\ndef get_optimizer(model):\n    return optim.AdamW([\n        {'params': model.features.parameters(), 'lr': 2e-5},\n        {'params': model.regressors.parameters(), 'lr': 5e-4}\n    ], weight_decay=1e-4) \n\ndef get_scheduler(optimizer):\n    return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6, verbose=True)\n\n# Data transformations\ndata_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(5),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\n# Training and evaluation\ndef train_model(model, train_dataloader, test_dataloader, num_epochs=20, lambda_param=0.0001, save_path='model_checkpoint.pth'):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    optimizer = get_optimizer(model)\n    scheduler = get_scheduler(optimizer)\n    best_mae = float('inf')\n    early_stop_patience = 15\n    no_improve_epochs = 0\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        for images, density_maps in train_dataloader:\n            images = images.to(device)\n            density_maps = density_maps.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images) \n            loss = negative_correlation_loss(outputs, density_maps, lambda_param)  \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n            optimizer.step() \n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n\n        model.eval()\n        mae, rmse = evaluate_model(model, test_dataloader)\n        if mae < best_mae:\n            best_mae = mae\n            best_rmse = rmse\n            no_improve_epochs = 0\n            torch.save(model.state_dict(), save_path)\n        else:\n            no_improve_epochs += 1\n\n        if no_improve_epochs >= early_stop_patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n        scheduler.step()\n    print(f\" MAE: {best_mae}, Validation RMSE: {best_rmse}\")\ndef custom_collate(batch):\n    max_height = max([item[0].shape[1] for item in batch])\n    max_width = max([item[0].shape[2] for item in batch])\n\n    resized_images = []\n    resized_density_maps = []\n    for image, density_map in batch:\n        image = F.interpolate(image.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False)\n        image = image.squeeze(0)\n\n        density_map = F.interpolate(density_map.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False)\n        density_map = density_map.squeeze(0)\n\n        resized_images.append(image)\n        resized_density_maps.append(density_map)\n\n    return torch.stack(resized_images), torch.stack(resized_density_maps)\ndef evaluate_model(model, dataloader):\n    model.eval()\n    mae, rmse = 0.0, 0.0\n    with torch.no_grad():\n        for images, density_maps in dataloader:\n            images = images.to(device)\n            density_maps = density_maps.to(device)\n\n            outputs = model(images)\n            avg_output = sum(outputs) / len(outputs)\n\n            mae += torch.abs(avg_output.sum() - density_maps.sum()).item()\n            rmse += ((avg_output.sum() - density_maps.sum()) ** 2).item()\n\n    mae /= len(dataloader)\n    rmse = (rmse / len(dataloader)) ** 0.5\n    return mae, rmse\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Dataset paths (Update these to your local paths)\ntrain_image_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_B/train_data/images'\ntrain_density_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_B/train_data/ground-truth-h5'\n\ntest_image_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_B/test_data/images'\ntest_density_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_B/test_data/ground-truth-h5'\n\ntrain_dataset = CrowdDataset(train_image_dir, train_density_dir, transform=data_transforms)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=custom_collate)\n\ntest_dataset = CrowdDataset(test_image_dir, test_density_dir, transform=data_transforms)\ntest_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=custom_collate)\n\nmodel = DConvNet_v1_with_Attention(pretrained=True)\n\ntrain_model(model, train_dataloader, test_dataloader, num_epochs=20, lambda_param=0.0001, save_path='model_checkpoint.pth')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-06T05:21:09.208763Z","iopub.execute_input":"2024-11-06T05:21:09.209140Z","iopub.status.idle":"2024-11-06T05:30:33.100460Z","shell.execute_reply.started":"2024-11-06T05:21:09.209102Z","shell.execute_reply":"2024-11-06T05:30:33.099297Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 211MB/s]  \n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 0.024170016511343418\nEpoch 2/20, Loss: 0.0025231755757704377\nEpoch 3/20, Loss: 0.0006837312786956318\nEpoch 4/20, Loss: 0.000286365827487316\nEpoch 5/20, Loss: 0.0001554872874112334\nEpoch 6/20, Loss: 9.792719523829874e-05\nEpoch 7/20, Loss: 7.235763008793583e-05\nEpoch 8/20, Loss: 5.945373181020841e-05\nEpoch 9/20, Loss: 5.209415303397691e-05\nEpoch 10/20, Loss: 4.976085558155319e-05\nEpoch 11/20, Loss: 4.8361351327912414e-05\nEpoch 12/20, Loss: 4.759423762152437e-05\nEpoch 13/20, Loss: 4.493484328122577e-05\nEpoch 14/20, Loss: 4.138555563258706e-05\nEpoch 15/20, Loss: 4.642780600988772e-05\nEpoch 16/20, Loss: 4.780822229804471e-05\nEpoch 17/20, Loss: 5.586158082223846e-05\nEpoch 18/20, Loss: 5.886226113943849e-05\nEpoch 19/20, Loss: 6.0089974194852406e-05\nEpoch 20/20, Loss: 5.2746081055374814e-05\n MAE: 20.02309754528577, Validation RMSE: 22.565459738731498\n","output_type":"stream"}],"execution_count":1}]}