{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":497705,"sourceType":"datasetVersion","datasetId":233465}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport numpy as np\nfrom PIL import Image\nimport h5py\nfrom sklearn.model_selection import KFold\nimport torch.nn.functional as F\nimport random\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False  # This will make training slower, but more reproducible\n\nset_seed(38)  # Use any integer as seed\n# Custom Dataset Class to Handle Images and Precomputed Density Maps from the Same Folder\nclass CrowdDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        # Assuming that .jpg files correspond to images and .h5 to density maps\n        self.image_filenames = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        # Get the image filename\n        img_filename = self.image_filenames[idx]\n        # Derive the corresponding density map filename by replacing .jpg with .h5\n        density_filename = img_filename.replace('.jpg', '.h5')\n\n        # Construct full paths to the image and the density map\n        img_path = os.path.join(self.data_dir, img_filename)\n        density_path = os.path.join(self.data_dir, density_filename)\n\n        # Load image\n        image = Image.open(img_path).convert('RGB')\n\n        # Load density map from .h5 file\n        with h5py.File(density_path, 'r') as hf:\n            density_map = np.array(hf['density'])\n\n        # Apply transforms to the image\n        if self.transform:\n            image = self.transform(image)\n\n        # Convert density map to a tensor\n        density_map = torch.from_numpy(density_map).unsqueeze(0).float()  # Add channel dimension\n\n        return image, density_map\n\n\n# D-ConvNet-v1 Implementation\nclass DConvNet_v1(nn.Module):\n    def __init__(self, pretrained=True, num_regressors=3):\n        super(DConvNet_v1, self).__init__()\n\n        # Load the VGG16 model\n        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n\n        # Modify the feature extractor (up to conv4_3)\n        self.features = nn.Sequential(*list(vgg16.features.children())[:23])  # Up to conv4_3\n\n        # Modify the fourth pooling layer (set stride to 1)\n        self.features.add_module('pool4', nn.MaxPool2d(kernel_size=2, stride=1, padding=0))\n\n        # Add dilated convolutions in place of the fifth pooling layer\n        self.features.add_module('dilated_conv5_1', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('relu5_1', nn.ReLU(inplace=True))\n        self.features.add_module('dilated_conv5_2', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('relu5_2', nn.ReLU(inplace=True))\n        self.features.add_module('dilated_conv5_3', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n        self.features.add_module('relu5_3', nn.ReLU(inplace=True))\n\n        # Define group convolutional layers for regression with dropout\n        self.regressors = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(512, 64, kernel_size=1, groups=64),  # Group convolution\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.3),  # Added dropout layer\n                nn.Conv2d(64, 1, kernel_size=1)  # Final 1x1 convolution to get the density map\n            ) for _ in range(num_regressors)\n        ])\n\n    def forward(self, x):\n        # Pass through the modified VGG16 feature extractor\n        x = self.features(x)\n        \n        # Apply the group convolutional regressors\n        outputs = [regressor(x) for regressor in self.regressors]\n        return outputs\n\n\n# Loss Function (Euclidean Loss + Negative Correlation)\ndef negative_correlation_loss(outputs, target, lambda_param=0.001):\n    mse_loss = nn.MSELoss()\n    target = target.to(outputs[0].device)\n\n    # Upsample to match the target size\n    total_mse = sum([mse_loss(F.interpolate(output, size=target.shape[2:], mode='bilinear', align_corners=False), target) for output in outputs]) / len(outputs)\n\n    # Calculate pairwise correlations between regressors\n    correlations = []\n    for i in range(len(outputs)):\n        for j in range(i + 1, len(outputs)):\n            o_i = outputs[i].view(-1)\n            o_j = outputs[j].view(-1)\n            corr = torch.corrcoef(torch.stack([o_i, o_j]))[0, 1]\n            correlations.append(corr)\n\n    correlation_penalty = -sum(correlations) / (len(correlations) + 1e-8) if correlations else 0\n    return total_mse + lambda_param * correlation_penalty\n\n\n# Optimizer Function\ndef get_optimizer(model):\n    return optim.SGD([\n        {'params': model.features.parameters(), 'lr': 1e-5},  # Smaller learning rate for feature extraction layers\n        {'params': model.regressors.parameters(), 'lr': 1e-4}  # Larger learning rate for regressor layers\n    ], momentum=0.9, weight_decay=1e-3)\n\n\n# Learning Rate Scheduler\ndef get_scheduler(optimizer):\n    return optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n\n\n# Custom collate function to resize images and density maps in each batch\ndef custom_collate(batch):\n    max_height = max([item[0].shape[1] for item in batch])\n    max_width = max([item[0].shape[2] for item in batch])\n\n    resized_images = []\n    resized_density_maps = []\n    for image, density_map in batch:\n        image = F.interpolate(image.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False)\n        image = image.squeeze(0)\n\n        density_map = F.interpolate(density_map.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False)\n        density_map = density_map.squeeze(0)\n\n        resized_images.append(image)\n        resized_density_maps.append(density_map)\n\n    return torch.stack(resized_images), torch.stack(resized_density_maps)\n\n\n# Training Loop with Model Saving and Evaluation\ndef train_model(model, train_dataloader, test_dataloader, num_epochs=40, lambda_param=0.001, save_path='model_checkpoint.pth'):\n    model = model.to(device)\n    optimizer = get_optimizer(model)\n    scheduler = get_scheduler(optimizer)\n    best_mae = float('inf')\n    early_stop_patience = 5\n    no_improve_epochs = 0\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        for images, density_maps in train_dataloader:\n            images = images.to(device)\n            density_maps = density_maps.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = negative_correlation_loss(outputs, density_maps, lambda_param)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n\n        model.eval()\n        mae, rmse = evaluate_model(model, test_dataloader)\n\n        if mae < best_mae:\n            best_mae = mae\n            no_improve_epochs = 0\n            torch.save(model.state_dict(), f'{save_path}_epoch_{epoch+1}.pth')\n            print(f\"Model saved as {save_path}_epoch_{epoch+1}.pth\")\n        else:\n            no_improve_epochs += 1\n\n        if no_improve_epochs >= early_stop_patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n        scheduler.step()\n\n\n# Evaluation Function\ndef evaluate_model(model, dataloader):\n    model.eval()\n    mae, rmse = 0.0, 0.0\n    with torch.no_grad():\n        for images, density_maps in dataloader:\n            images = images.to(device)\n            density_maps = density_maps.to(device)\n\n            outputs = model(images)\n            avg_output = sum(outputs) / len(outputs)\n\n            mae += torch.abs(avg_output.sum() - density_maps.sum()).item()\n            rmse += ((avg_output.sum() - density_maps.sum()) ** 2).item()\n\n    mae /= len(dataloader)\n    rmse = (rmse / len(dataloader)) ** 0.5\n    print(f\"Validation MAE: {mae}, Validation RMSE: {rmse}\")\n    return mae, rmse\n\n\n# Cross-Validation Function\ndef cross_validate_model(model, dataset, num_epochs=40, k_folds=5, lambda_param=0.001):\n    kfold = KFold(n_splits=k_folds, shuffle=True)\n    fold_results = {'mae': [], 'rmse': []}\n    \n    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n        print(f'FOLD {fold + 1}/{k_folds}')\n        print('--------------------------------')\n\n        train_subset = Subset(dataset, train_ids)\n        test_subset = Subset(dataset, test_ids)\n\n        train_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=4, collate_fn=custom_collate)\n        test_dataloader = DataLoader(test_subset, batch_size=4, shuffle=False, num_workers=4, collate_fn=custom_collate)\n\n        model_fold = DConvNet_v1(pretrained=True)\n        model_fold = model_fold.to(device)\n\n        train_model(model_fold, train_dataloader, test_dataloader, num_epochs, lambda_param, save_path=f'model_checkpoint_fold_{fold + 1}.pth')\n\n        mae, rmse = evaluate_model(model_fold, test_dataloader)\n        fold_results['mae'].append(mae)\n        fold_results['rmse'].append(rmse)\n\n        print(f'Fold {fold + 1} Results - MAE: {mae}, RMSE: {rmse}')\n        print('--------------------------------')\n\n    avg_mae = np.mean(fold_results['mae'])\n    avg_rmse = np.mean(fold_results['rmse'])\n\n    print(f'\\nCross-validation Results:')\n    print(f'Average MAE: {avg_mae}')\n    print(f'Average RMSE: {avg_rmse}')\n\n\n# Configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Path to the folder containing both images and density maps\ndata_dir = '/kaggle/input/ucf-cc-50-with-people-density-map/UCF_CC_50'\n\n# Data augmentation and normalization\ndata_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\n# Initialize the Dataset\ndataset = CrowdDataset(data_dir, transform=data_transforms)\n\n# Initialize the Model\nmodel = DConvNet_v1(pretrained=True)\n\n# Perform Cross-Validation\ncross_validate_model(model, dataset, num_epochs=40, k_folds=5, lambda_param=0.001)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-22T05:11:12.627500Z","iopub.execute_input":"2024-10-22T05:11:12.627960Z","iopub.status.idle":"2024-10-22T05:15:48.882697Z","shell.execute_reply.started":"2024-10-22T05:11:12.627907Z","shell.execute_reply":"2024-10-22T05:15:48.881459Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 205MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"FOLD 1/5\n--------------------------------\nEpoch 1/40, Loss: 0.018124780617654324\nValidation MAE: 319.08880106608075, Validation RMSE: 334.2753989118977\nModel saved as model_checkpoint_fold_1.pth_epoch_1.pth\nEpoch 2/40, Loss: 0.017865943349897863\nValidation MAE: 320.05943298339844, Validation RMSE: 334.94648107166563\nEpoch 3/40, Loss: 0.017252483405172825\nValidation MAE: 322.5976257324219, Validation RMSE: 337.35300232409867\nEpoch 4/40, Loss: 0.01659717094153166\nValidation MAE: 323.42189534505206, Validation RMSE: 338.66101037455826\nEpoch 5/40, Loss: 0.015933844074606896\nValidation MAE: 323.5528208414714, Validation RMSE: 338.9162731920304\nEpoch 6/40, Loss: 0.015420432668179274\nValidation MAE: 324.3854064941406, Validation RMSE: 339.65415928068165\nEarly stopping at epoch 6\nValidation MAE: 324.7680969238281, Validation RMSE: 340.0236492234768\nFold 1 Results - MAE: 324.7680969238281, RMSE: 340.0236492234768\n--------------------------------\nFOLD 2/5\n--------------------------------\nEpoch 1/40, Loss: 0.020615462958812714\nValidation MAE: 391.8563181559245, Validation RMSE: 412.155144674005\nModel saved as model_checkpoint_fold_2.pth_epoch_1.pth\nEpoch 2/40, Loss: 0.020176697894930838\nValidation MAE: 389.7989857991536, Validation RMSE: 410.0626714677282\nModel saved as model_checkpoint_fold_2.pth_epoch_2.pth\nEpoch 3/40, Loss: 0.019646140560507775\nValidation MAE: 388.0724792480469, Validation RMSE: 408.2952076163765\nModel saved as model_checkpoint_fold_2.pth_epoch_3.pth\nEpoch 4/40, Loss: 0.01913528759032488\nValidation MAE: 385.4958801269531, Validation RMSE: 405.82981314780625\nModel saved as model_checkpoint_fold_2.pth_epoch_4.pth\nEpoch 5/40, Loss: 0.01849455330520868\nValidation MAE: 383.0101674397786, Validation RMSE: 403.2879112035428\nModel saved as model_checkpoint_fold_2.pth_epoch_5.pth\nEpoch 6/40, Loss: 0.018022451363503934\nValidation MAE: 383.27150472005206, Validation RMSE: 403.10948290549226\nEpoch 7/40, Loss: 0.017708496004343034\nValidation MAE: 381.64405822753906, Validation RMSE: 401.75171450059383\nModel saved as model_checkpoint_fold_2.pth_epoch_7.pth\nEpoch 8/40, Loss: 0.01741138156503439\nValidation MAE: 380.8614959716797, Validation RMSE: 400.8703454412245\nModel saved as model_checkpoint_fold_2.pth_epoch_8.pth\nEpoch 9/40, Loss: 0.017306175641715528\nValidation MAE: 379.2524159749349, Validation RMSE: 399.4084558235258\nModel saved as model_checkpoint_fold_2.pth_epoch_9.pth\nEpoch 10/40, Loss: 0.01699194498360157\nValidation MAE: 379.4256642659505, Validation RMSE: 399.35822702128394\nEpoch 11/40, Loss: 0.016844761557877064\nValidation MAE: 378.2920735677083, Validation RMSE: 398.0358449363156\nModel saved as model_checkpoint_fold_2.pth_epoch_11.pth\nEpoch 12/40, Loss: 0.016750891320407392\nValidation MAE: 377.1712341308594, Validation RMSE: 397.0746922153522\nModel saved as model_checkpoint_fold_2.pth_epoch_12.pth\nEpoch 13/40, Loss: 0.016661774925887586\nValidation MAE: 377.29454549153644, Validation RMSE: 397.05361630232005\nEpoch 14/40, Loss: 0.016499813832342624\nValidation MAE: 377.01513671875, Validation RMSE: 396.74951592335515\nModel saved as model_checkpoint_fold_2.pth_epoch_14.pth\nEpoch 15/40, Loss: 0.01624850193038583\nValidation MAE: 376.1994222005208, Validation RMSE: 396.36960274365975\nModel saved as model_checkpoint_fold_2.pth_epoch_15.pth\nEpoch 16/40, Loss: 0.016228580847382545\nValidation MAE: 377.0196889241536, Validation RMSE: 396.8588300511699\nEpoch 17/40, Loss: 0.016230462305247783\nValidation MAE: 376.27516682942706, Validation RMSE: 396.14444736369\nEpoch 18/40, Loss: 0.01620161337777972\nValidation MAE: 375.63006591796875, Validation RMSE: 395.4538251990785\nModel saved as model_checkpoint_fold_2.pth_epoch_18.pth\nEpoch 19/40, Loss: 0.016032273415476084\nValidation MAE: 375.88820393880206, Validation RMSE: 395.78134746347\nEpoch 20/40, Loss: 0.015978656709194183\nValidation MAE: 375.0699768066406, Validation RMSE: 395.1286018237185\nModel saved as model_checkpoint_fold_2.pth_epoch_20.pth\nEpoch 21/40, Loss: 0.015964339766651393\nValidation MAE: 375.057861328125, Validation RMSE: 394.6406263403875\nModel saved as model_checkpoint_fold_2.pth_epoch_21.pth\nEpoch 22/40, Loss: 0.015949968807399274\nValidation MAE: 375.5211486816406, Validation RMSE: 395.2949732610025\nEpoch 23/40, Loss: 0.015848163422197102\nValidation MAE: 374.1969350179036, Validation RMSE: 393.99509071867465\nModel saved as model_checkpoint_fold_2.pth_epoch_23.pth\nEpoch 24/40, Loss: 0.015890784468501805\nValidation MAE: 375.11440022786456, Validation RMSE: 395.0834069376997\nEpoch 25/40, Loss: 0.01593197798356414\nValidation MAE: 373.7237854003906, Validation RMSE: 393.7309453061401\nModel saved as model_checkpoint_fold_2.pth_epoch_25.pth\nEpoch 26/40, Loss: 0.015871864277869463\nValidation MAE: 374.75294494628906, Validation RMSE: 394.6701945929031\nEpoch 27/40, Loss: 0.01577834114432335\nValidation MAE: 374.7547149658203, Validation RMSE: 394.42522461488187\nEpoch 28/40, Loss: 0.015818698983639478\nValidation MAE: 373.9247233072917, Validation RMSE: 393.73572725454653\nEpoch 29/40, Loss: 0.01585952267050743\nValidation MAE: 373.6209411621094, Validation RMSE: 393.34277884139004\nModel saved as model_checkpoint_fold_2.pth_epoch_29.pth\nEpoch 30/40, Loss: 0.015796054061502217\nValidation MAE: 374.658940633138, Validation RMSE: 394.60325704275425\nEpoch 31/40, Loss: 0.01576481545343995\nValidation MAE: 373.706298828125, Validation RMSE: 393.52406050699955\nEpoch 32/40, Loss: 0.015791005361825226\nValidation MAE: 373.397705078125, Validation RMSE: 393.21060276269253\nModel saved as model_checkpoint_fold_2.pth_epoch_32.pth\nEpoch 33/40, Loss: 0.01576105076819658\nValidation MAE: 373.6271718343099, Validation RMSE: 393.363729183945\nEpoch 34/40, Loss: 0.015705106779932976\nValidation MAE: 373.48504638671875, Validation RMSE: 393.1739536293064\nEpoch 35/40, Loss: 0.015677639935165643\nValidation MAE: 374.1787109375, Validation RMSE: 394.1688579044096\nEpoch 36/40, Loss: 0.015703698340803384\nValidation MAE: 374.3351643880208, Validation RMSE: 394.36541522953894\nEpoch 37/40, Loss: 0.01582723520696163\nValidation MAE: 373.96311950683594, Validation RMSE: 393.67979698237417\nEarly stopping at epoch 37\nValidation MAE: 374.52545166015625, Validation RMSE: 394.35922445996704\nFold 2 Results - MAE: 374.52545166015625, RMSE: 394.35922445996704\n--------------------------------\nFOLD 3/5\n--------------------------------\nEpoch 1/40, Loss: 0.0032370158238336443\nValidation MAE: 483.5931447347005, Validation RMSE: 536.6177212653731\nModel saved as model_checkpoint_fold_3.pth_epoch_1.pth\nEpoch 2/40, Loss: 0.0031608413672074677\nValidation MAE: 481.7938537597656, Validation RMSE: 534.8119442899609\nModel saved as model_checkpoint_fold_3.pth_epoch_2.pth\nEpoch 3/40, Loss: 0.0031435036566108466\nValidation MAE: 482.8168690999349, Validation RMSE: 535.5464602326379\nEpoch 4/40, Loss: 0.0031191721092909573\nValidation MAE: 481.9800771077474, Validation RMSE: 534.8412342306203\nEpoch 5/40, Loss: 0.0031043168855831027\nValidation MAE: 482.4431559244792, Validation RMSE: 535.5850085955466\nEpoch 6/40, Loss: 0.0030472780112177134\nValidation MAE: 481.8700358072917, Validation RMSE: 534.7075421190151\nEpoch 7/40, Loss: 0.0030450120102614164\nValidation MAE: 481.34266153971356, Validation RMSE: 533.9499826616487\nModel saved as model_checkpoint_fold_3.pth_epoch_7.pth\nEpoch 8/40, Loss: 0.0030390279134735465\nValidation MAE: 481.5163319905599, Validation RMSE: 534.1523732162013\nEpoch 9/40, Loss: 0.002996743773110211\nValidation MAE: 481.29307047526044, Validation RMSE: 534.1527742115702\nModel saved as model_checkpoint_fold_3.pth_epoch_9.pth\nEpoch 10/40, Loss: 0.002977689588442445\nValidation MAE: 482.1172180175781, Validation RMSE: 534.8300608701952\nEpoch 11/40, Loss: 0.00299965285230428\nValidation MAE: 480.4810434977214, Validation RMSE: 532.8939148453157\nModel saved as model_checkpoint_fold_3.pth_epoch_11.pth\nEpoch 12/40, Loss: 0.0029413013020530344\nValidation MAE: 481.2167561848958, Validation RMSE: 534.0628742440663\nEpoch 13/40, Loss: 0.002960091596469283\nValidation MAE: 481.8661142985026, Validation RMSE: 534.8079130924018\nEpoch 14/40, Loss: 0.002948568668216467\nValidation MAE: 481.2974548339844, Validation RMSE: 534.1255917726264\nEpoch 15/40, Loss: 0.0029516844544559715\nValidation MAE: 480.9254404703776, Validation RMSE: 533.8607308539529\nEpoch 16/40, Loss: 0.00295016523450613\nValidation MAE: 481.9518636067708, Validation RMSE: 534.636232443768\nEarly stopping at epoch 16\nValidation MAE: 482.4425455729167, Validation RMSE: 535.616944343351\nFold 3 Results - MAE: 482.4425455729167, RMSE: 535.616944343351\n--------------------------------\nFOLD 4/5\n--------------------------------\nEpoch 1/40, Loss: 0.014888945780694485\nValidation MAE: 734.4922485351562, Validation RMSE: 773.1402686447008\nModel saved as model_checkpoint_fold_4.pth_epoch_1.pth\nEpoch 2/40, Loss: 0.01456143194809556\nValidation MAE: 728.8967488606771, Validation RMSE: 767.4557533391312\nModel saved as model_checkpoint_fold_4.pth_epoch_2.pth\nEpoch 3/40, Loss: 0.014018598850816489\nValidation MAE: 726.2445882161459, Validation RMSE: 765.1163106025123\nModel saved as model_checkpoint_fold_4.pth_epoch_3.pth\nEpoch 4/40, Loss: 0.013563993852585554\nValidation MAE: 718.8603719075521, Validation RMSE: 757.6304893108073\nModel saved as model_checkpoint_fold_4.pth_epoch_4.pth\nEpoch 5/40, Loss: 0.013115001935511827\nValidation MAE: 714.9042561848959, Validation RMSE: 753.3975405897385\nModel saved as model_checkpoint_fold_4.pth_epoch_5.pth\nEpoch 6/40, Loss: 0.0127555419690907\nValidation MAE: 711.8180338541666, Validation RMSE: 750.324256294126\nModel saved as model_checkpoint_fold_4.pth_epoch_6.pth\nEpoch 7/40, Loss: 0.012528002820909023\nValidation MAE: 709.7888997395834, Validation RMSE: 748.6531587346261\nModel saved as model_checkpoint_fold_4.pth_epoch_7.pth\nEpoch 8/40, Loss: 0.01236767005175352\nValidation MAE: 707.8481648763021, Validation RMSE: 746.478922173962\nModel saved as model_checkpoint_fold_4.pth_epoch_8.pth\nEpoch 9/40, Loss: 0.012135375197976828\nValidation MAE: 704.2451171875, Validation RMSE: 742.7644240156723\nModel saved as model_checkpoint_fold_4.pth_epoch_9.pth\nEpoch 10/40, Loss: 0.011931047495454549\nValidation MAE: 703.5895182291666, Validation RMSE: 741.7569514335541\nModel saved as model_checkpoint_fold_4.pth_epoch_10.pth\nEpoch 11/40, Loss: 0.011818428337574006\nValidation MAE: 701.6781412760416, Validation RMSE: 740.2620172389413\nModel saved as model_checkpoint_fold_4.pth_epoch_11.pth\nEpoch 12/40, Loss: 0.011603651847690345\nValidation MAE: 700.7214558919271, Validation RMSE: 740.1237227653226\nModel saved as model_checkpoint_fold_4.pth_epoch_12.pth\nEpoch 13/40, Loss: 0.011457287520170212\nValidation MAE: 698.8434448242188, Validation RMSE: 736.9588692729059\nModel saved as model_checkpoint_fold_4.pth_epoch_13.pth\nEpoch 14/40, Loss: 0.011454989202320576\nValidation MAE: 697.2663167317709, Validation RMSE: 736.1349549335366\nModel saved as model_checkpoint_fold_4.pth_epoch_14.pth\nEpoch 15/40, Loss: 0.011376442946493625\nValidation MAE: 695.6932373046875, Validation RMSE: 734.300435930916\nModel saved as model_checkpoint_fold_4.pth_epoch_15.pth\nEpoch 16/40, Loss: 0.011339420452713966\nValidation MAE: 696.1470540364584, Validation RMSE: 735.2847981394692\nEpoch 17/40, Loss: 0.01122747380286455\nValidation MAE: 695.4146728515625, Validation RMSE: 734.6794617098898\nModel saved as model_checkpoint_fold_4.pth_epoch_17.pth\nEpoch 18/40, Loss: 0.011173916514962912\nValidation MAE: 694.6473999023438, Validation RMSE: 734.0009863185925\nModel saved as model_checkpoint_fold_4.pth_epoch_18.pth\nEpoch 19/40, Loss: 0.011078427173197269\nValidation MAE: 693.1231892903646, Validation RMSE: 731.3000659499036\nModel saved as model_checkpoint_fold_4.pth_epoch_19.pth\nEpoch 20/40, Loss: 0.01116953818127513\nValidation MAE: 695.1988321940104, Validation RMSE: 733.9370431674187\nEpoch 21/40, Loss: 0.011070810724049807\nValidation MAE: 693.1327921549479, Validation RMSE: 731.8586676230595\nEpoch 22/40, Loss: 0.011048537213355303\nValidation MAE: 694.6712036132812, Validation RMSE: 733.36479099195\nEpoch 23/40, Loss: 0.011064415052533149\nValidation MAE: 693.3012084960938, Validation RMSE: 732.2532718716022\nEpoch 24/40, Loss: 0.011090350057929754\nValidation MAE: 690.8529256184896, Validation RMSE: 729.3818051667682\nModel saved as model_checkpoint_fold_4.pth_epoch_24.pth\nEpoch 25/40, Loss: 0.010958460438996554\nValidation MAE: 692.4780069986979, Validation RMSE: 731.7712814352128\nEpoch 26/40, Loss: 0.01108373301103711\nValidation MAE: 692.485595703125, Validation RMSE: 731.9248275096289\nEpoch 27/40, Loss: 0.011017049383372068\nValidation MAE: 693.2843627929688, Validation RMSE: 731.8288129462609\nEpoch 28/40, Loss: 0.010945376195013522\nValidation MAE: 690.5346069335938, Validation RMSE: 729.4832753109104\nModel saved as model_checkpoint_fold_4.pth_epoch_28.pth\nEpoch 29/40, Loss: 0.01094869039952755\nValidation MAE: 692.5287068684896, Validation RMSE: 731.6172902378675\nEpoch 30/40, Loss: 0.010885850712656974\nValidation MAE: 692.8976236979166, Validation RMSE: 732.3686920135696\nEpoch 31/40, Loss: 0.01086394339799881\nValidation MAE: 692.1539713541666, Validation RMSE: 731.192878823137\nEpoch 32/40, Loss: 0.010877111833542586\nValidation MAE: 689.3235270182291, Validation RMSE: 727.8098101782268\nModel saved as model_checkpoint_fold_4.pth_epoch_32.pth\nEpoch 33/40, Loss: 0.010863615013659001\nValidation MAE: 692.3477579752604, Validation RMSE: 731.7577261578224\nEpoch 34/40, Loss: 0.01082815434783697\nValidation MAE: 691.9483642578125, Validation RMSE: 730.9106049989971\nEpoch 35/40, Loss: 0.010862794145941735\nValidation MAE: 692.2561848958334, Validation RMSE: 730.4929271845598\nEpoch 36/40, Loss: 0.010843122471123933\nValidation MAE: 691.0223388671875, Validation RMSE: 729.7073007663187\nEpoch 37/40, Loss: 0.010758763924241067\nValidation MAE: 690.505126953125, Validation RMSE: 728.9671596112315\nEarly stopping at epoch 37\nValidation MAE: 691.6278279622396, Validation RMSE: 730.2622852156157\nFold 4 Results - MAE: 691.6278279622396, RMSE: 730.2622852156157\n--------------------------------\nFOLD 5/5\n--------------------------------\nEpoch 1/40, Loss: 0.004212398454546929\nValidation MAE: 262.6523081461589, Validation RMSE: 297.3033833608567\nModel saved as model_checkpoint_fold_5.pth_epoch_1.pth\nEpoch 2/40, Loss: 0.004143967293202877\nValidation MAE: 261.6599833170573, Validation RMSE: 296.48791650827303\nModel saved as model_checkpoint_fold_5.pth_epoch_2.pth\nEpoch 3/40, Loss: 0.004117185343056917\nValidation MAE: 262.74073282877606, Validation RMSE: 296.8033479870166\nEpoch 4/40, Loss: 0.004027090733870864\nValidation MAE: 261.76885986328125, Validation RMSE: 296.04174730235826\nEpoch 5/40, Loss: 0.003945648856461048\nValidation MAE: 261.73868052164715, Validation RMSE: 296.2508823387001\nEpoch 6/40, Loss: 0.003901949757710099\nValidation MAE: 262.11918385823566, Validation RMSE: 296.9785600623701\nEpoch 7/40, Loss: 0.0038534906459972264\nValidation MAE: 261.34886678059894, Validation RMSE: 295.9778777782464\nModel saved as model_checkpoint_fold_5.pth_epoch_7.pth\nEpoch 8/40, Loss: 0.0038385191466659306\nValidation MAE: 262.3559799194336, Validation RMSE: 297.2125361869291\nEpoch 9/40, Loss: 0.003757043764926493\nValidation MAE: 262.8508809407552, Validation RMSE: 297.25711537863845\nEpoch 10/40, Loss: 0.003737523057498038\nValidation MAE: 262.40628306070965, Validation RMSE: 297.2452669058276\nEpoch 11/40, Loss: 0.00371170099824667\nValidation MAE: 261.91664123535156, Validation RMSE: 296.80801821123225\nEpoch 12/40, Loss: 0.0037203959887847303\nValidation MAE: 260.9572575887044, Validation RMSE: 295.5577219846228\nModel saved as model_checkpoint_fold_5.pth_epoch_12.pth\nEpoch 13/40, Loss: 0.003724135784432292\nValidation MAE: 262.7146174112956, Validation RMSE: 297.23216700125244\nEpoch 14/40, Loss: 0.0036674704868346454\nValidation MAE: 261.7951838175456, Validation RMSE: 296.4469776315404\nEpoch 15/40, Loss: 0.003631428605876863\nValidation MAE: 261.447509765625, Validation RMSE: 296.2010106954046\nEpoch 16/40, Loss: 0.0036337944911792874\nValidation MAE: 260.9365946451823, Validation RMSE: 295.4004697017254\nModel saved as model_checkpoint_fold_5.pth_epoch_16.pth\nEpoch 17/40, Loss: 0.0036264615366235374\nValidation MAE: 262.3980255126953, Validation RMSE: 296.66284716715523\nEpoch 18/40, Loss: 0.0036250169621780516\nValidation MAE: 261.7077331542969, Validation RMSE: 295.9214785789889\nEpoch 19/40, Loss: 0.003622149000875652\nValidation MAE: 262.06141662597656, Validation RMSE: 296.32144581737936\nEpoch 20/40, Loss: 0.00358203942887485\nValidation MAE: 262.5446421305339, Validation RMSE: 297.12429527463405\nEpoch 21/40, Loss: 0.0036052638199180363\nValidation MAE: 261.9630432128906, Validation RMSE: 296.8411148424987\nEarly stopping at epoch 21\nValidation MAE: 261.9925486246745, Validation RMSE: 296.28128570737755\nFold 5 Results - MAE: 261.9925486246745, RMSE: 296.28128570737755\n--------------------------------\n\nCross-validation Results:\nAverage MAE: 427.071294148763\nAverage RMSE: 459.30867778995764\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}