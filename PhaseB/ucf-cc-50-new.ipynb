{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":497705,"sourceType":"datasetVersion","datasetId":233465}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport numpy as np\nfrom PIL import Image\nimport h5py\nfrom sklearn.model_selection import KFold\nimport torch.nn.functional as F\nimport random\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(38)\n\nclass CrowdDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.image_filenames = [f for f in os.listdir(data_dir) if f.endswith('.jpg')]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        img_filename = self.image_filenames[idx]\n        density_filename = img_filename.replace('.jpg', '.h5')\n        img_path = os.path.join(self.data_dir, img_filename)\n        density_path = os.path.join(self.data_dir, density_filename)\n        image = Image.open(img_path).convert('RGB')\n\n        with h5py.File(density_path, 'r') as hf:\n            density_map = np.array(hf['density'])\n\n        if self.transform:\n            image = self.transform(image)\n\n        density_map = torch.from_numpy(density_map).unsqueeze(0).float()\n        return image, density_map\n\nclass MultiScaleAttentionDConvNet(nn.Module):\n    def __init__(self, pretrained=True, num_regressors=3):\n        super(MultiScaleAttentionDConvNet, self).__init__()\n        \n        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n        self.features = nn.Sequential(*list(vgg16.features.children())[:23])\n        self.features.add_module('pool4', nn.MaxPool2d(kernel_size=2, stride=1, padding=0))\n\n        # Multi-Scale Convolution Layers\n        self.multi_scale_layers = nn.ModuleList([\n            nn.Conv2d(512, 128, kernel_size=3, padding=1),\n            nn.Conv2d(512, 128, kernel_size=5, padding=2),\n            nn.Conv2d(512, 128, kernel_size=7, padding=3)\n        ])\n        \n        # Channel adjustment layer to bring concatenated multi-scale features to 512 channels\n        self.channel_adjust = nn.Conv2d(384, 512, kernel_size=1)\n\n        # Channel Attention Layers\n        self.channel_fc1 = nn.Linear(512, 256)\n        self.channel_fc2 = nn.Linear(256, 512)\n        \n        # Spatial Attention Layers\n        self.spatial_conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n\n        # Ensemble of Regressors\n        self.regressors = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(512, 64, kernel_size=1, groups=64),\n                nn.ReLU(inplace=True),\n                nn.Dropout(0.3),\n                nn.Conv2d(64, 1, kernel_size=1)\n            ) for _ in range(num_regressors)\n        ])\n        self.regressors.apply(self.init_weights)\n        \n    def forward(self, x):\n        x = self.features(x)\n        \n        # Multi-Scale Feature Extraction\n        multi_scale_feats = [layer(x) for layer in self.multi_scale_layers]\n        x = torch.cat(multi_scale_feats, dim=1)\n        \n        # Adjust channels to match expected input for regressors\n        x = self.channel_adjust(x)\n        \n        # Channel Attention\n        gap = F.adaptive_avg_pool2d(x, (1, 1)).view(x.shape[0], -1)\n        channel_weights = torch.sigmoid(self.channel_fc2(F.relu(self.channel_fc1(gap))))\n        channel_weights = channel_weights.view(x.shape[0], -1, 1, 1)\n        x = x * channel_weights\n        \n        # Spatial Attention\n        avg_pool = torch.mean(x, dim=1, keepdim=True)\n        max_pool, _ = torch.max(x, dim=1, keepdim=True)\n        spatial_attention_map = torch.sigmoid(self.spatial_conv(torch.cat([avg_pool, max_pool], dim=1)))\n        x = x * spatial_attention_map\n        \n        # Ensemble Regressors\n        outputs = [regressor(x) for regressor in self.regressors]\n        return outputs\n\n    def init_weights(self, m):\n        if isinstance(m, nn.Conv2d):\n            torch.nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                m.bias.data.fill_(0.01)\n\ndef negative_correlation_loss(outputs, target, lambda_param=0.0001):  # Reduced lambda\n    mse_loss = nn.MSELoss()\n    target = target.to(outputs[0].device)\n    total_mse = sum([mse_loss(F.interpolate(output, size=target.shape[2:], mode='bilinear', align_corners=False), target) for output in outputs]) / len(outputs)\n\n    correlations = []\n    for i in range(len(outputs)):\n        for j in range(i + 1, len(outputs)):\n            o_i = outputs[i].view(-1)\n            o_j = outputs[j].view(-1)\n            corr = torch.corrcoef(torch.stack([o_i, o_j]))[0, 1]\n            correlations.append(corr)\n\n    correlation_penalty = -sum(correlations) / (len(correlations) + 1e-8) if correlations else 0\n    return total_mse + lambda_param * correlation_penalty\n\ndef get_optimizer(model):\n    return optim.SGD([\n        {'params': model.features.parameters(), 'lr': 1e-6},   # Reduced learning rate\n        {'params': model.regressors.parameters(), 'lr': 1e-4}  # Reduced learning rate\n    ], momentum=0.9, weight_decay=1e-4)  # Reduced weight decay for better generalization\n\ndef get_scheduler(optimizer):\n    return optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n\ndef custom_collate(batch):\n    max_height = max([item[0].shape[1] for item in batch])\n    max_width = max([item[0].shape[2] for item in batch])\n\n    resized_images = []\n    resized_density_maps = []\n    for image, density_map in batch:\n        image = F.interpolate(image.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False).squeeze(0)\n        density_map = F.interpolate(density_map.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False).squeeze(0)\n        resized_images.append(image)\n        resized_density_maps.append(density_map)\n\n    return torch.stack(resized_images), torch.stack(resized_density_maps)\n\ndef train_model(model, train_dataloader, test_dataloader, num_epochs=40, lambda_param=0.0001, early_stop_patience=10, save_path='model_checkpoint.pth'):\n    model = model.to(device)\n    optimizer = get_optimizer(model)\n    scheduler = get_scheduler(optimizer)\n    best_mae = float('inf')\n    no_improve_epochs = 0\n\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        model.train()\n        for images, density_maps in train_dataloader:\n            images, density_maps = images.to(device), density_maps.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = negative_correlation_loss(outputs, density_maps, lambda_param)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n\n        # Validate and update learning rate based on validation loss\n        model.eval()\n        mae, rmse = evaluate_model(model, test_dataloader)\n        scheduler.step(mae)  # Pass MAE for ReduceLROnPlateau scheduler\n\n        if mae < best_mae:\n            best_mae = mae\n            no_improve_epochs = 0\n        else:\n            no_improve_epochs += 1\n\n        if no_improve_epochs >= early_stop_patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\ndef evaluate_model(model, dataloader):\n    model.eval()\n    mae, rmse = 0.0, 0.0\n    with torch.no_grad():\n        for images, density_maps in dataloader:\n            images, density_maps = images.to(device), density_maps.to(device)\n            outputs = model(images)\n            avg_output = sum(outputs) / len(outputs)\n            mae += torch.abs(avg_output.sum() - density_maps.sum()).item()\n            rmse += ((avg_output.sum() - density_maps.sum()) ** 2).item()\n\n    mae /= len(dataloader)\n    rmse = (rmse / len(dataloader)) ** 0.5\n    print(f\"Validation MAE: {mae}, Validation RMSE: {rmse}\")\n    return mae, rmse\n\ndef cross_validate_model(model, dataset, num_epochs=40, k_folds=5, lambda_param=0.0001):\n    kfold = KFold(n_splits=k_folds, shuffle=True)\n    fold_results = {'mae': [], 'rmse': []}\n    \n    for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n        print(f'FOLD {fold + 1}/{k_folds}')\n        \n        train_subset = Subset(dataset, train_ids)\n        test_subset = Subset(dataset, test_ids)\n        \n        train_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=4, collate_fn=custom_collate)\n        test_dataloader = DataLoader(test_subset, batch_size=4, shuffle=False, num_workers=4, collate_fn=custom_collate)\n        \n        model_fold = MultiScaleAttentionDConvNet(pretrained=True).to(device)\n        train_model(model_fold, train_dataloader, test_dataloader, num_epochs, lambda_param, save_path=f'model_checkpoint_fold_{fold + 1}.pth')\n\n        # Evaluate the model on the test set and store the results\n        mae, rmse = evaluate_model(model_fold, test_dataloader)\n        fold_results['mae'].append(mae)\n        fold_results['rmse'].append(rmse)\n\n        print(f'Fold {fold + 1} Results - MAE: {mae}, RMSE: {rmse}')\n        print('--------------------------------')\n\n    avg_mae = np.mean(fold_results['mae'])\n    avg_rmse = np.mean(fold_results['rmse'])\n\n    print(f'\\nCross-validation Results:')\n    print(f'Average MAE: {avg_mae}')\n    print(f'Average RMSE: {avg_rmse}')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndata_dir = '/kaggle/input/ucf-cc-50-with-people-density-map/UCF_CC_50'\ndata_transforms = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n])\n\ndataset = CrowdDataset(data_dir, transform=data_transforms)\nmodel = MultiScaleAttentionDConvNet(pretrained=True)\ncross_validate_model(model, dataset, num_epochs=40, k_folds=5, lambda_param=0.0001)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T06:57:22.231334Z","iopub.execute_input":"2024-11-04T06:57:22.231756Z","iopub.status.idle":"2024-11-04T06:59:15.506376Z","shell.execute_reply.started":"2024-11-04T06:57:22.231711Z","shell.execute_reply":"2024-11-04T06:59:15.505232Z"}},"outputs":[{"name":"stdout","text":"FOLD 1/5\nEpoch 1/40, Loss: 0.007221816061064601\nValidation MAE: 302.6132405598958, Validation RMSE: 311.87342183969446\nEpoch 2/40, Loss: 0.007267665350809693\nValidation MAE: 307.37939453125, Validation RMSE: 317.4242855915932\nEpoch 3/40, Loss: 0.007228999445214867\nValidation MAE: 310.72999572753906, Validation RMSE: 321.8594292350311\nEpoch 4/40, Loss: 0.00710424673743546\nValidation MAE: 307.99704996744794, Validation RMSE: 318.7870811764492\nEpoch 5/40, Loss: 0.007141839154064656\nValidation MAE: 309.07818603515625, Validation RMSE: 319.7103221733178\nEpoch 6/40, Loss: 0.0071951454039663075\nValidation MAE: 313.4729512532552, Validation RMSE: 323.8518843063137\nEpoch 7/40, Loss: 0.006858250126242638\nValidation MAE: 306.6170399983724, Validation RMSE: 315.64043072841076\nEpoch 8/40, Loss: 0.006915181735530495\nValidation MAE: 306.7333170572917, Validation RMSE: 316.18964139091696\nEpoch 9/40, Loss: 0.007010513171553612\nValidation MAE: 314.3286590576172, Validation RMSE: 324.9697421812273\nEpoch 10/40, Loss: 0.006929253973066807\nValidation MAE: 317.4202473958333, Validation RMSE: 328.1779104166164\nEpoch 11/40, Loss: 0.006792274862527847\nValidation MAE: 313.1515808105469, Validation RMSE: 323.3890422637261\nEarly stopping at epoch 11\nValidation MAE: 317.48566182454425, Validation RMSE: 328.5307986735541\nFold 1 Results - MAE: 317.48566182454425, RMSE: 328.5307986735541\n--------------------------------\nFOLD 2/5\nEpoch 1/40, Loss: 0.008177690673619509\nValidation MAE: 270.3492609659831, Validation RMSE: 292.7726004477138\nEpoch 2/40, Loss: 0.008929059305228294\nValidation MAE: 280.7955322265625, Validation RMSE: 304.0945143089768\nEpoch 3/40, Loss: 0.009171728044748306\nValidation MAE: 274.9109598795573, Validation RMSE: 299.1863505087242\nEpoch 4/40, Loss: 0.008749877149239183\nValidation MAE: 282.2283655802409, Validation RMSE: 304.4105931875428\nEpoch 5/40, Loss: 0.008403499517589808\nValidation MAE: 281.6554667154948, Validation RMSE: 304.32476056974616\nEpoch 6/40, Loss: 0.009170912159606814\nValidation MAE: 282.11477915445965, Validation RMSE: 304.68795138855455\nEpoch 7/40, Loss: 0.008696567174047232\nValidation MAE: 282.2724024454753, Validation RMSE: 305.53344370355626\nEpoch 8/40, Loss: 0.009191559907048941\nValidation MAE: 271.2255045572917, Validation RMSE: 292.02361212590756\nEpoch 9/40, Loss: 0.009057933883741497\nValidation MAE: 276.8558044433594, Validation RMSE: 301.04241259278984\nEpoch 10/40, Loss: 0.008490800904110075\nValidation MAE: 277.29835001627606, Validation RMSE: 301.70413664334853\nEpoch 11/40, Loss: 0.008211658941581846\nValidation MAE: 285.8311513264974, Validation RMSE: 309.5044110650929\nEarly stopping at epoch 11\nValidation MAE: 283.2891082763672, Validation RMSE: 306.380881896483\nFold 2 Results - MAE: 283.2891082763672, RMSE: 306.380881896483\n--------------------------------\nFOLD 3/5\nEpoch 1/40, Loss: 0.006579924747347832\nValidation MAE: 458.91156005859375, Validation RMSE: 511.8645437983731\nEpoch 2/40, Loss: 0.006392064318060875\nValidation MAE: 462.5327911376953, Validation RMSE: 513.3310665196162\nEpoch 3/40, Loss: 0.006584760034456849\nValidation MAE: 460.0300649007161, Validation RMSE: 511.7076185176046\nEpoch 4/40, Loss: 0.006646368047222495\nValidation MAE: 465.8367970784505, Validation RMSE: 518.0843633541581\nEpoch 5/40, Loss: 0.006609178334474564\nValidation MAE: 461.72715250651044, Validation RMSE: 514.2989343564775\nEpoch 6/40, Loss: 0.006403701147064567\nValidation MAE: 462.06292215983075, Validation RMSE: 516.1828938931594\nEpoch 7/40, Loss: 0.006321139167994261\nValidation MAE: 465.73597717285156, Validation RMSE: 518.9453930458789\nEpoch 8/40, Loss: 0.006506362510845065\nValidation MAE: 466.12958272298175, Validation RMSE: 519.026804286357\nEpoch 9/40, Loss: 0.006441853567957878\nValidation MAE: 460.59959920247394, Validation RMSE: 512.3361749084205\nEpoch 10/40, Loss: 0.0062961714342236515\nValidation MAE: 465.0783386230469, Validation RMSE: 517.9941814395296\nEpoch 11/40, Loss: 0.006295532174408436\nValidation MAE: 465.2113749186198, Validation RMSE: 518.9891611093225\nEarly stopping at epoch 11\nValidation MAE: 465.4662322998047, Validation RMSE: 519.1104552150165\nFold 3 Results - MAE: 465.4662322998047, RMSE: 519.1104552150165\n--------------------------------\nFOLD 4/5\nEpoch 1/40, Loss: 0.00989741664379835\nValidation MAE: 445.0292561848958, Validation RMSE: 495.2467556262568\nEpoch 2/40, Loss: 0.009250066662207245\nValidation MAE: 442.75677998860675, Validation RMSE: 494.31406335362\nEpoch 3/40, Loss: 0.009518423676490783\nValidation MAE: 445.8594106038411, Validation RMSE: 495.6004065987369\nEpoch 4/40, Loss: 0.009453456243500113\nValidation MAE: 445.86693318684894, Validation RMSE: 496.2146306140284\nEpoch 5/40, Loss: 0.00971579789184034\nValidation MAE: 440.0171203613281, Validation RMSE: 488.201895885128\nEpoch 6/40, Loss: 0.009280863893218338\nValidation MAE: 442.6283365885417, Validation RMSE: 492.90141345447233\nEpoch 7/40, Loss: 0.009659922448918223\nValidation MAE: 448.5123036702474, Validation RMSE: 498.7882164765256\nEpoch 8/40, Loss: 0.009009929257445037\nValidation MAE: 446.75672403971356, Validation RMSE: 495.43482716574334\nEpoch 9/40, Loss: 0.00899754543788731\nValidation MAE: 444.7904103597005, Validation RMSE: 494.6390018867295\nEpoch 10/40, Loss: 0.008989281812682747\nValidation MAE: 444.66269938151044, Validation RMSE: 491.54372447719913\nEpoch 11/40, Loss: 0.008835692564025522\nValidation MAE: 445.9942932128906, Validation RMSE: 494.9812667435843\nEpoch 12/40, Loss: 0.00882329223677516\nValidation MAE: 449.8099619547526, Validation RMSE: 500.12631737725354\nEpoch 13/40, Loss: 0.008773977542296052\nValidation MAE: 445.72377522786456, Validation RMSE: 496.2543844264754\nEpoch 14/40, Loss: 0.008790954016149044\nValidation MAE: 450.8457539876302, Validation RMSE: 498.9131468752853\nEpoch 15/40, Loss: 0.008958938345313073\nValidation MAE: 447.6813659667969, Validation RMSE: 497.1240623157698\nEarly stopping at epoch 15\nValidation MAE: 448.3592987060547, Validation RMSE: 498.85873136005813\nFold 4 Results - MAE: 448.3592987060547, RMSE: 498.85873136005813\n--------------------------------\nFOLD 5/5\nEpoch 1/40, Loss: 0.006240598997101188\nValidation MAE: 163.81076304117838, Validation RMSE: 200.94297335750676\nEpoch 2/40, Loss: 0.0061804268974810835\nValidation MAE: 173.59248224894205, Validation RMSE: 207.7258586544272\nEpoch 3/40, Loss: 0.006015382381156087\nValidation MAE: 162.26099650065103, Validation RMSE: 195.66581274666171\nEpoch 4/40, Loss: 0.005834678839892149\nValidation MAE: 162.55841573079428, Validation RMSE: 198.4999746775831\nEpoch 5/40, Loss: 0.005988887557759881\nValidation MAE: 167.03924306233725, Validation RMSE: 202.2067186956779\nEpoch 6/40, Loss: 0.005802370305173099\nValidation MAE: 167.82937622070312, Validation RMSE: 201.95371058405593\nEpoch 7/40, Loss: 0.006119462614879012\nValidation MAE: 171.4141934712728, Validation RMSE: 206.3168372655855\nEpoch 8/40, Loss: 0.005881943739950657\nValidation MAE: 170.25797526041666, Validation RMSE: 207.31515334705279\nEpoch 9/40, Loss: 0.005481112748384476\nValidation MAE: 174.31377919514975, Validation RMSE: 209.92755783598568\nEpoch 10/40, Loss: 0.005799909355118871\nValidation MAE: 168.32742055257162, Validation RMSE: 203.6013215162636\nEpoch 11/40, Loss: 0.005656939232721925\nValidation MAE: 171.01996358235678, Validation RMSE: 204.98347438790137\nEpoch 12/40, Loss: 0.005468166945502162\nValidation MAE: 169.80291239420572, Validation RMSE: 209.01644018298992\nEpoch 13/40, Loss: 0.005934766447171569\nValidation MAE: 170.24109268188477, Validation RMSE: 204.86082433349307\nEarly stopping at epoch 13\nValidation MAE: 173.50106557210287, Validation RMSE: 207.82083779189247\nFold 5 Results - MAE: 173.50106557210287, RMSE: 207.82083779189247\n--------------------------------\n\nCross-validation Results:\nAverage MAE: 337.6202733357747\nAverage RMSE: 372.14034098740086\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}