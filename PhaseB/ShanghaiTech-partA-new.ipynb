{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T19:45:10.632685Z",
     "iopub.status.busy": "2024-11-05T19:45:10.632118Z",
     "iopub.status.idle": "2024-11-05T19:50:12.922149Z",
     "shell.execute_reply": "2024-11-05T19:50:12.920857Z",
     "shell.execute_reply.started": "2024-11-05T19:45:10.632645Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.02208491520335277\n",
      "Epoch 2/20, Loss: 0.0030232950191323954\n",
      "Epoch 3/20, Loss: 0.0016824603561932843\n",
      "Epoch 4/20, Loss: 0.0012741122643152873\n",
      "Epoch 5/20, Loss: 0.001051342796999961\n",
      "Epoch 6/20, Loss: 0.0009314766161454221\n",
      "Epoch 7/20, Loss: 0.0008602293627336621\n",
      "Epoch 8/20, Loss: 0.0008088776236400008\n",
      "Epoch 9/20, Loss: 0.0007808357415099938\n",
      "Epoch 10/20, Loss: 0.0007622865649561087\n",
      "Epoch 11/20, Loss: 0.0007314315414987504\n",
      "Epoch 12/20, Loss: 0.0007537360341909031\n",
      "Epoch 13/20, Loss: 0.0007199080133189758\n",
      "Epoch 14/20, Loss: 0.0007239933265373111\n",
      "Epoch 15/20, Loss: 0.0006558102136477828\n",
      "Epoch 16/20, Loss: 0.000628331598903363\n",
      "Early stopping at epoch 16\n",
      " MAE: 159.84397357443103, Validation RMSE: 182.14207352321003\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import h5py\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(38) \n",
    "\n",
    "class CrowdDataset(Dataset):\n",
    "    def __init__(self, image_dir, density_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.density_dir = density_dir\n",
    "        self.image_filenames = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        density_path = os.path.join(self.density_dir, self.image_filenames[idx].replace('.jpg', '.h5'))\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        with h5py.File(density_path, 'r') as hf:\n",
    "            density_map = np.array(hf['density'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        density_map = torch.from_numpy(density_map).unsqueeze(0).float() \n",
    "\n",
    "        return image, density_map\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleAttention, self).__init__()\n",
    "        \n",
    "        # Multi-Scale Feature Extraction with improved convolutions\n",
    "        self.conv1 = nn.Conv2d(512, 128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(512, 128, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(512, 128, kernel_size=7, padding=3)\n",
    "        self.conv4 = nn.Conv2d(512, 128, kernel_size=1, padding=0)  # Added additional scale with 1x1 convolution\n",
    "        \n",
    "        # Channel Attention Mechanism\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.global_max_pool = nn.AdaptiveMaxPool2d(1)  # Added max pooling for improved channel representation\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 512)\n",
    "        \n",
    "        # Enhanced Spatial Attention Mechanism\n",
    "        self.spatial_conv1 = nn.Conv2d(512, 512, kernel_size=1)  # Modified to keep the number of channels consistent\n",
    "        self.spatial_conv2 = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-Scale Feature Extraction\n",
    "        f1 = self.conv1(x)\n",
    "        f2 = self.conv2(x)\n",
    "        f3 = self.conv3(x)\n",
    "        f4 = self.conv4(x)\n",
    "        f_multi = torch.cat([f1, f2, f3, f4], dim=1)\n",
    "        \n",
    "        # Channel Attention\n",
    "        avg_pool = self.global_avg_pool(f_multi).view(f_multi.size(0), -1)\n",
    "        max_pool = self.global_max_pool(f_multi).view(f_multi.size(0), -1)\n",
    "        channel_weights = torch.sigmoid(self.fc2(F.relu(self.fc1(avg_pool + max_pool)))).view(f_multi.size(0), 512, 1, 1)\n",
    "        f_channel = f_multi * channel_weights\n",
    "        \n",
    "        # Enhanced Spatial Attention\n",
    "        f_spatial = F.relu(self.spatial_conv1(f_channel))\n",
    "        avg_out = torch.mean(f_spatial, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(f_spatial, dim=1, keepdim=True)\n",
    "        spatial_attention = torch.sigmoid(self.spatial_conv2(torch.cat([avg_out, max_out], dim=1)))\n",
    "        f_attention = f_spatial * spatial_attention\n",
    "        \n",
    "        # Residual Connection\n",
    "        f_attention += f_channel  # Add residual connection to retain original features\n",
    "        \n",
    "        return f_attention\n",
    "\n",
    "class DConvNet_v1_with_Attention(nn.Module):\n",
    "    def __init__(self, pretrained=True, num_regressors=5):\n",
    "        super(DConvNet_v1_with_Attention, self).__init__()\n",
    "\n",
    "        vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        self.features = nn.Sequential(*list(vgg16.features.children())[:23])\n",
    "        self.features.add_module('pool4', nn.MaxPool2d(kernel_size=2, stride=1, padding=0))\n",
    "        self.features.add_module('dilated_conv5_1', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n",
    "        self.features.add_module('bn5_1', nn.BatchNorm2d(512))\n",
    "        self.features.add_module('relu5_1', nn.ReLU(inplace=True))\n",
    "        self.features.add_module('dilated_conv5_2', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n",
    "        self.features.add_module('bn5_2', nn.BatchNorm2d(512))\n",
    "        self.features.add_module('relu5_2', nn.ReLU(inplace=True))\n",
    "        self.features.add_module('dilated_conv5_3', nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2))\n",
    "        self.features.add_module('bn5_3', nn.BatchNorm2d(512))\n",
    "        self.features.add_module('relu5_3', nn.ReLU(inplace=True))\n",
    "\n",
    "        # Unfreeze additional VGG layers for fine-tuning\n",
    "        for param in self.features[:10].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.attention = MultiScaleAttention()\n",
    "\n",
    "        self.regressors = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(512, 64, kernel_size=1, groups=64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.05),  # Further reduced dropout rate to improve learning\n",
    "                nn.Conv2d(64, 1, kernel_size=1)  \n",
    "            ) for _ in range(num_regressors)\n",
    "        ])\n",
    "        self.regressors.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.attention(x)\n",
    "        outputs = [regressor(x) for regressor in self.regressors]\n",
    "        return outputs\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')  # Switched to Kaiming initialization for better convergence\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.fill_(0.01)\n",
    "\n",
    "def negative_correlation_loss(outputs, target, lambda_param=0.0001):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    target = target.to(outputs[0].device)\n",
    "\n",
    "    total_mse = sum([mse_loss(F.interpolate(output, size=target.shape[2:], mode='bilinear', align_corners=False), target) for output in outputs]) / len(outputs)\n",
    "    correlations = []\n",
    "    for i in range(len(outputs)):\n",
    "        for j in range(i + 1, len(outputs)):\n",
    "            o_i = outputs[i].view(-1)\n",
    "            o_j = outputs[j].view(-1)\n",
    "            corr = torch.corrcoef(torch.stack([o_i, o_j]))[0, 1]\n",
    "            correlations.append(corr)\n",
    "\n",
    "    correlation_penalty = abs(sum(correlations)) / (len(correlations) + 1e-8) if correlations else 0  # Use absolute value\n",
    "    return total_mse + lambda_param * correlation_penalty\n",
    "\n",
    "# Training and Evaluation Code\n",
    "\n",
    "def get_optimizer(model):\n",
    "    return optim.AdamW([\n",
    "        {'params': model.features.parameters(), 'lr': 2e-5},  # Increased learning rate for features to speed up convergence\n",
    "        {'params': model.regressors.parameters(), 'lr': 5e-4}  # Increased learning rate for regressors\n",
    "    ], weight_decay=1e-4) \n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "    return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6, verbose=True)  # Switched to Cosine Annealing for better learning rate scheduling\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),  # Reduced rotation angle for even less aggressive augmentation\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Adjusted scale for better image representation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def custom_collate(batch):\n",
    "    max_height = max([item[0].shape[1] for item in batch])\n",
    "    max_width = max([item[0].shape[2] for item in batch])\n",
    "\n",
    "    resized_images = []\n",
    "    resized_density_maps = []\n",
    "    for image, density_map in batch:\n",
    "        image = F.interpolate(image.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False)\n",
    "        image = image.squeeze(0)\n",
    "\n",
    "        density_map = F.interpolate(density_map.unsqueeze(0), size=(max_height, max_width), mode='bilinear', align_corners=False)\n",
    "        density_map = density_map.squeeze(0)\n",
    "\n",
    "        resized_images.append(image)\n",
    "        resized_density_maps.append(density_map)\n",
    "\n",
    "    return torch.stack(resized_images), torch.stack(resized_density_maps)\n",
    "\n",
    "def train_model(model, train_dataloader, test_dataloader, num_epochs=20, lambda_param=0.0001, save_path='model_checkpoint.pth'):\n",
    "    model = model.to(device)\n",
    "    optimizer = get_optimizer(model)\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "    best_mae = float('inf')\n",
    "    early_stop_patience = 15  # Increased patience for early stopping to allow more training epochs\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for images, density_maps in train_dataloader:\n",
    "            images = images.to(device)\n",
    "            density_maps = density_maps.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images) \n",
    "            loss = negative_correlation_loss(outputs, density_maps, lambda_param)  \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Increased gradient clipping threshold\n",
    "            optimizer.step() \n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}\")\n",
    "\n",
    "        model.eval()\n",
    "        mae, rmse = evaluate_model(model, test_dataloader)\n",
    "        if mae < best_mae:\n",
    "            best_mae = mae\n",
    "            best_rmse = rmse\n",
    "            no_improve_epochs = 0\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs >= early_stop_patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "    print(f\" MAE: {best_mae}, Validation RMSE: {best_rmse}\")\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    mae, rmse = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, density_maps in dataloader:\n",
    "            images = images.to(device)\n",
    "            density_maps = density_maps.to(device)\n",
    "\n",
    "            outputs = model(images)  \n",
    "            avg_output = sum(outputs) / len(outputs)\n",
    "\n",
    "            mae += torch.abs(avg_output.sum() - density_maps.sum()).item()\n",
    "            rmse += ((avg_output.sum() - density_maps.sum()) ** 2).item()\n",
    "\n",
    "    mae /= len(dataloader)\n",
    "    rmse = (rmse / len(dataloader)) ** 0.5\n",
    "    return mae, rmse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Dataset paths (Update these to your local paths)\n",
    "train_image_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/train_data/images'\n",
    "train_density_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/train_data/ground-truth-h5'\n",
    "\n",
    "test_image_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/test_data/images'\n",
    "test_density_dir = '/kaggle/input/shanghaitech-with-people-density-map/ShanghaiTech/part_A/test_data/ground-truth-h5'\n",
    "\n",
    "train_dataset = CrowdDataset(train_image_dir, train_density_dir, transform=data_transforms)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=custom_collate)\n",
    "\n",
    "test_dataset = CrowdDataset(test_image_dir, test_density_dir, transform=data_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=4, collate_fn=custom_collate)\n",
    "\n",
    "model = DConvNet_v1_with_Attention(pretrained=True)\n",
    "\n",
    "train_model(model, train_dataloader, test_dataloader, num_epochs=20, lambda_param=0.0001, save_path='model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 233357,
     "sourceId": 497494,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
